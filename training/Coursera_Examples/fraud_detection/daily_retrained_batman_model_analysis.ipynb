{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily retraining for Fake Property Detection: Impact, Stability, and Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This analysis is a first look at the predictions that our daily retrained Batman model [1], which gets generated every day by our automated retrained pipeline [2]. This model runs in production in \"dry-running mode\", which means that this model makes predictions for each property registration, but we are not (yet) using these predictions in our business decisions. \n",
    "\n",
    "Our business decisions are still being made by a model that we trained once on 2021-02-26 [3]. This model is completely identical both in feature set, model architecture, and in hyperparameters as [1], and the only difference is that [1] is retrained/redeployed every day using fresh data.\n",
    "\n",
    "We aim to look into the following **research questions**:\n",
    "\n",
    "- **RQ 1**: does the daily-retrained-model (i.e., model [1]) make better predictions than the stale model (i.e., model [3])?\n",
    "- **RQ 2**: does it happen that with automated-daily-retraining we on some days we deploy a \"bad\" model?\n",
    "- **RQ 3**: does the distribution of the model scores of the daily-retrained-model (i.e., model [1]) have a higher variance from day-to-day than the distribution of model scores of the stale model (i.e., model [3])? If so, we might have a challenge in setting thresholds for the daily-retrained-model.\n",
    "\n",
    "This analysis is structured as follows:\n",
    "\n",
    "- **Data preparation**, which is a prerequisite to be able to answer all three questions\n",
    "- **The effectiveness of daily-retraining on Batman**, aims to answer RQ 1\n",
    "- **The consistency of the effectiveness of daily-retraining on Batman**, aims to answer RQ 2\n",
    "- **Analysis of Volume-stability**, aims to answer RQ 3\n",
    "- **Conclusions**\n",
    "\n",
    "This analyis contains is pretty long and detailed, and it mixes code for experimental setup with textual interpretation of plots and results. To quickly find only the results and their interpretation, these all start with the indicator \"**Observation:**\", which allows you to quickly skim through this analysis by CTRL-F-ing on that word.\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [https://ml.booking.com/model/partner_fraud_preopening_batman_20210226_daily_retrained](https://ml.booking.com/model/partner_fraud_preopening_batman_20210226_daily_retrained)\n",
    "\n",
    "[2] [https://gitlab.booking.com/core/machine-learning-platform/model-building/training-pipelines/-/tree/master/partner_fraud_preopening_batman_20210226_daily_retrained](https://gitlab.booking.com/core/machine-learning-platform/model-building/training-pipelines/-/tree/master/partner_fraud_preopening_batman_20210226_daily_retrained)\n",
    "\n",
    "[3] [https://ml.booking.com/model/partner_fraud_preopening_batman_20210226](https://ml.booking.com/model/partner_fraud_preopening_batman_20210226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf, types\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"REFRESH TABLE counterfraud.batman_model_instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "We filter the data on the time range 2021-04-02 (the day when we started to make online predictions with the daily-retrained-model) to yesterday. Which gives us almost two weeks of model predictions. In the last few days, our labels might not be fully mature yet (i.e., we don't know all the fraud yet). However, gradually over time, we have gotten much better at catching fake properties fast, and we nowadays find ~90% of the fake properties within just a few days (as found in [this analysis](https://analysis.booking.com/post/75280930.kp))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = (\n",
    "    spark.table(\"counterfraud.batman_model_instances\")\n",
    "    .where(sf.col(\"signup_finished_at\") >= \"2021-04-02\")\n",
    "    .where(sf.col(\"signup_finished_at\") < \"2021-05-17\")\n",
    "    .withColumn(\"registration_date\", sf.col(\"signup_finished_at\").substr(0, 10))\n",
    "    .select(\"property_id\", \"is_fake_hotel\", \"registration_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_parse_schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"score\", types.FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Obtain the predictions of the stale model\n",
    "batman_stale_predictions = (\n",
    "    spark.table(\"dbimports.sectools_batmanmodelpredictionlog\")\n",
    "    .where(sf.col(\"model_instance\") == \"partner_fraud_preopening_batman_20210226\")\n",
    "    .where(sf.col(\"source_system\") == \"RS\")\n",
    "    .select(\"property_id\", \"prediction\")\n",
    "    .withColumn(\"prediction\", sf.from_json(\"prediction\", json_parse_schema))\n",
    "    .select(sf.col(\"property_id\"), sf.col(\"prediction.*\"))\n",
    "    .withColumnRenamed(\"score\", \"stale_prediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances_w_batman = instances.join(batman_stale_predictions, on=\"property_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Obtain the predictions of the daily-retrained-model model\n",
    "batman_retrained_predictions = (\n",
    "    spark.table(\"dbimports.sectools_batmanmodelpredictionlog\")\n",
    "    .where(sf.col(\"model_instance\") == \"partner_fraud_preopening_batman_20210226_daily_retrained\")\n",
    "    .where(sf.col(\"source_system\") == \"RS\")\n",
    "    .select(\"property_id\", \"prediction\")\n",
    "    .withColumn(\"prediction\", sf.from_json(\"prediction\", json_parse_schema))\n",
    "    .select(sf.col(\"property_id\"), sf.col(\"prediction.*\"))\n",
    "    .withColumnRenamed(\"score\", \"retrained_prediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now join both the predictions of the stale model and those of the daily-retrained-model into the instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batman_predictions = (\n",
    "    instances_w_batman\n",
    "    .join(batman_retrained_predictions, on=\"property_id\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first look at daily-retrained-model predictions\n",
    "\n",
    "Let's see what kind of properties daily-retrained-model detects that are currently not yet known fakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    batman_predictions\n",
    "    .where(sf.col(\"is_fake_hotel\") == 0)\n",
    "    .withColumn(\"prediction_delta\", sf.col(\"retrained_prediction\") - sf.col(\"stale_prediction\"))\n",
    "    .orderBy(\"prediction_delta\", ascending=False)\n",
    ").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have a lot of hotels where the fraud-risk of the hotel is much higher in the retrained model than in the stale model. These might be types of fraud that are of a type that is new, and simply did not yet exist in the training data at the time that the stale model was trained. I have given a list of 50 properties (all where the score of the new model was more than 0.4 higher than that of the stale model) to Amir Kalter from our fraud operations team to manually investigate.\n",
    "\n",
    "His analysis findings are shown [here](https://docs.google.com/spreadsheets/d/18Is0vlZzbaJIDN3MvvF4gfOTN_kC0Tt09x72XHJBMM0/edit#gid=0), where he:\n",
    "\n",
    "- closed 30 out of these 50 properties because they were fake\n",
    "- found 2 false positives that were non-fake\n",
    "- for 18 properties he was not able to find conclusive evidence, but suspected most of those to be fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status = spark.table(\"dbimports.acquisition_property\").selectExpr(\"id as property_id\", \"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning of these statusses:\n",
    "\n",
    "- Status 140 is spam registration\n",
    "- Status 11 are \"test\"-properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    batman_predictions\n",
    "    .join(status, on=\"property_id\", how=\"left\")\n",
    "    .where(sf.col(\"is_fake_hotel\") == 0)\n",
    "    # The score bucket is the first decimal of the fraud-risk, e.g., score 0.743 is score_bucket 7\n",
    "    .withColumn(\"score_bucket\", sf.col(\"retrained_prediction\").substr(3,1))\n",
    "    .groupBy(\"score_bucket\")\n",
    "    .pivot(\"status\")\n",
    "    .agg(sf.sum(sf.lit(1)).alias(\"count\"))\n",
    "    .orderBy(\"score_bucket\")\n",
    "    .fillna(0)\n",
    ").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Spam hotels (140) tend be fairly uniformly distibuted over the buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_pd = (\n",
    "    batman_predictions\n",
    "    .join(status, on=\"property_id\", how=\"left\")\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have earlier identified a data issue and found that all spam-properties are non-fake properties that are wrongly put into that status because of some hacky processing of the registrations team. We exlude those properties, and the test properties from further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonspam_nontest_properties = properties_pd[(properties_pd[\"status\"] != 140) & (properties_pd[\"status\"] != 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonspam_nontest_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The effectiveness of daily-retraining on Batman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some basic visual analysis and plot the models of the stale production model against those of the daily retrained model and visually inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7), dpi=80)\n",
    "ax = sns.scatterplot(data=nonspam_nontest_properties, \n",
    "                x=\"retrained_prediction\", \n",
    "                y=\"stale_prediction\", \n",
    "                hue=\"is_fake_hotel\",\n",
    "                alpha=0.2\n",
    "               )\n",
    "ax.set(ylim=(0, 1), xlim=(0,1))\n",
    "X_plot = np.linspace(0.001, 10)\n",
    "Y_plot = X_plot\n",
    "plt.plot(X_plot, Y_plot, color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Visually, we see that:\n",
    "\n",
    "- It seems in this plot that **there is a lot of orange (fake) in the bottom right**: where the stale model gives a low score and the daily retrained model gives a high score.\n",
    "- The daily-retrained-model **seems to give higher scores than the lower scores**, since there are more data points below the red y=x-line than above it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze this more quantitatively, we run a logistic regression, predicting the fakeness of a hotel based on model scores of both the stale model and of the daily-retrained-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonspam_nontest_properties = nonspam_nontest_properties.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = smf.glm(\n",
    "    formula='is_fake_hotel ~ stale_prediction + retrained_prediction', \n",
    "    data=nonspam_nontest_properties,\n",
    "    family=sm.families.Binomial()\n",
    ")\n",
    "results1 = model1.fit()\n",
    "results1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "We see in the GLM regression that the daily retrained model is a strong predictor of whether the hotel is fake.\n",
    "\n",
    "We also see in the GLM regression that the 95%-CI of the stale-score is fully below zero, indicating that after knowing the score of the daily-retrained-model, it is the case that a *higher* score in the stale model makes the property *less likely* to be fake rather than more likely.\n",
    "\n",
    "This seems to suggest that the daily-retrained-model is a much better predictor of hotel fakeness than the stale model (remember, the stale model is what we currently run in production).\n",
    "\n",
    "We additionally look at the overall ROC-AUC of the daily-retrained-model and the stale model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(\n",
    "    nonspam_nontest_properties[\"is_fake_hotel\"],\n",
    "    nonspam_nontest_properties[\"stale_prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(\n",
    "    nonspam_nontest_properties[\"is_fake_hotel\"],\n",
    "    nonspam_nontest_properties[\"retrained_prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "The daily retraining increased the ROC-AUC of Batman from an average of 0.868 to 0.906 on average over a the two-week period.\n",
    "\n",
    "Based on this analysis we can answer RQ 1 positively: **the daily-retrained-model does indeed make better predictions than the stale model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The consistency of the effectiveness of daily-retraining on Batman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we analyze the effect of the daily-retrained-model score **per day of registrations** instead of over the whole period. The hypothesis here is that over time the benefit of the daily-retrained-model over the stale model gets larger because the \"difference in data freshness\" grows over time. With this analysis we also hope to answer RQ2: if it is the case that on some days the daily-retrained-model deploys a bad model, we expect to that on some registration-days the stale model outperforms the daily-retrained-model.\n",
    "\n",
    "First, we parse the coefficient and its confidence intervals for the retrained_prediction from the logistic regression statsmodels model object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals of the contributions of daily-retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retrained_mean_coeff = results1.params.T[\"retrained_prediction\"]\n",
    "print(retrained_mean_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retrained_lower_coeff, retrained_upper_coeff = results1.conf_int(alpha=0.05, cols=None).T[\"retrained_prediction\"].to_list()\n",
    "print(retrained_lower_coeff, retrained_upper_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there numbers are indeed identical to the summary table above.\n",
    "\n",
    "We now repeat the analysis per day, to see how the daily-retrained-model coefficients trend over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "registration_dates = set(nonspam_nontest_properties[\"registration_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a method to get the coefficient and the CI for all the predictions of a single day of registrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coeffs_for_day(reg_date):\n",
    "    date_results = smf.glm(\n",
    "        formula='is_fake_hotel ~ stale_prediction + retrained_prediction', \n",
    "        data=nonspam_nontest_properties[nonspam_nontest_properties[\"registration_date\"] == reg_date],\n",
    "        family=sm.families.Binomial()\n",
    "    ).fit()\n",
    "    mean = date_results.params.T[\"retrained_prediction\"]\n",
    "    lower, upper = date_results.conf_int(alpha=0.05, cols=None).T[\"retrained_prediction\"].to_list()\n",
    "    \n",
    "    return mean, lower, upper, reg_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on a single day to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_coeffs_for_day(\"2021-04-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply this method on all the dates for which we have predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_per_date = [get_coeffs_for_day(reg_date) for reg_date in registration_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_per_date_pd = pd.DataFrame(\n",
    "    [{\"mean\": e1, \"CI_05\":e2, \"CI_95\": e3, \"reg_date\": e4} for e1, e2, e3, e4 in results_per_date]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start plotting the effects over time: for every registration date we plot the confidence interval of how much information is contained in the score of the daily-retrained-model **after** accounting for the information that is already contained in the score of the stale model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6), dpi=80)\n",
    "\n",
    "y_axis_name = \"Effect of daily-retrained model scores on the log-odds scale\"\n",
    "\n",
    "ax = sns.lineplot(\n",
    "    data=pd.melt(results_per_date_pd, ['reg_date'], value_name=y_axis_name), \n",
    "    x=\"reg_date\", \n",
    "    y=y_axis_name, \n",
    "    hue='variable',\n",
    ")\n",
    "ax.axhline(0, ls='--', color=\"black\")\n",
    "\n",
    "ax.set_title(\"The effect of daily-retrained model scores on hotel-fakeness over time (incremental, over knowing just the stale-model score)\")\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "Our hypothesis does seem to hold: over time the coefficients of the daily-retrained-model grow over time, which means that they become an increasingly strong signal of hotel-fakeness, and which also means that the stale models relatively becomes an increasingly weak signal of hotel-fakeness.\n",
    "\n",
    "Also note that there is not a single day in which the coefficients of the daily-retrained-model are weak. This suggests that so far after 11 consecutive days of automated retrain/deploys we haven't yet seen any bad deploys of the daily-retrained-model. We would like to monitor this a bit longer to be sure, but for now it does seem like **we can answer RQ2 negatively**: there is no evidence that we deploy a bad model on some days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of ROC-AUC per day\n",
    "Beyond analyzing the coefficient of the retrained_prediction in a Logistic Regression will also take another view on the contribution of daily-retraining on the performance of the model by calculating ROC-AUC for every day of registrations and plotting the daily-retrained-model against the stale model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_group(df):\n",
    "    y_hat = df.y_hat\n",
    "    y = df.is_fake_hotel\n",
    "    return metrics.roc_auc_score(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "melted_scores = pd.melt(\n",
    "    nonspam_nontest_properties.reset_index()[[\"registration_date\", \"stale_prediction\", \"retrained_prediction\", \"is_fake_hotel\"]],\n",
    "    id_vars=[\"registration_date\", \"is_fake_hotel\"],\n",
    "    value_name=\"y_hat\",\n",
    "    var_name=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = melted_scores.groupby([\"registration_date\", \"model\"]).apply(auc_group).reset_index()\n",
    "results.columns = [\"registration_date\", \"model\", \"ROC-AUC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.lineplot(\n",
    "    data=results, \n",
    "    x=\"registration_date\", \n",
    "    y=\"ROC-AUC\", \n",
    "    hue=\"model\",\n",
    ")\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "ax.axvline(\"2021-04-20\", color=\"red\")\n",
    "ax.text(\"2021-04-20\", 0.94, color=\"red\", s=\" <-- Lowered the Batman threshold,\\n       triggering more concept drift\")\n",
    "\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "The daily-retrained-model is consistently above the stale model for the whole time range, with the exception of 2021-04-17. This date was the day on which an old attack pattern (called the 2FA-attack) popped back up for one day, which was raised to the responsible product team and the loophole was closed, stopping the attack from the next day on.\n",
    "\n",
    "The fact that the daily-retrained-model model is consistently above the stale model gives further reinforcing evidence for **RQ1: daily-retraining does seem to positively impact the predictions**. Additionally, this gives evidence to **negatively answer RQ 2**, we have no evidence so far that daily retraining leads to bad model deployments on some days.\n",
    "\n",
    "It is also noticeable that we started becoming much more [aggressive with the Batman threshold on 2021-04-20](https://docs.google.com/document/d/1TMS4ohydf8E85xmvLjSOAqKfuSGqh1-gDcS2HCAX_gQ/edit#), the fraudsters started adapting their behavior more quickly to work around our defenses, and the ROC-AUC gap between our stale model and daily-retrained-model seems to have grown. This is shown figually in the figure above, and confirmed by the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results[\"is_before_lower_threshold\"] = results[\"registration_date\"] < \"2021-04-20\"\n",
    "\n",
    "results.groupby([\"is_before_lower_threshold\", \"model\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Volume-stability\n",
    "Here we investigate RQ3 and try to get insight into whether we expect to see risks on the volumes that we send to high-risk and to medium-risk as a consequence of daily retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first just eye-ball the mean and the median score per day of the stale model and of the daily-retrained-model to get a sense of how stable their score distributions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nonspam_nontest_properties.groupby(\"registration_date\").agg({\"stale_prediction\": [np.mean, np.median], \n",
    "                                                             \"retrained_prediction\": [np.mean, np.median],\n",
    "                                                             \"is_fake_hotel\": [np.mean]\n",
    "                                                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "The mean scores of the score distributions look slightly more wobbly that for the stale model. This might be acceptible if the daily-variability follows the fraud rate, i.e., if we send more properties to high-risk on the days that there also actually is more fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of volume-stability on the high-risk threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot how many properties are above the high risk threshold per day, and **how stable this volume is**. \n",
    "For the stale model we analyze this at threshold 0.5 (current product threshold for high risk)\n",
    "For the daily-retrained model we analyze this at a 0.6, because it seems to give slightly higher scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_risk_results = nonspam_nontest_properties.groupby(\"registration_date\").agg(\n",
    "    stale_prediction=pd.NamedAgg(column='stale_prediction', aggfunc=lambda x: (x > 0.5).sum()),\n",
    "    retrained_prediction=pd.NamedAgg(column='retrained_prediction', aggfunc=lambda x: (x > 0.6).sum()),\n",
    "    fraud_rate=pd.NamedAgg(column='is_fake_hotel', aggfunc=lambda x: x.mean()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "y_axis_name = \"Number of high-risk properties per day\"\n",
    "ax = sns.lineplot(\n",
    "    data=pd.melt(high_risk_results.reset_index()[[\"registration_date\",\n",
    "                                                  \"stale_prediction\",\n",
    "                                                  \"retrained_prediction\"]],\n",
    "                 ['registration_date'],\n",
    "                 value_name=y_axis_name\n",
    "                ), \n",
    "    x=\"registration_date\", \n",
    "    y=y_axis_name, \n",
    "    hue='variable',\n",
    ")\n",
    "\n",
    "ax.set_title(\"The number of high-risk properties per day, according to daily-retrained model and the stale model.\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set(ylim=(0, 100))\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(60)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax3 = sns.lineplot(data=high_risk_results[[\"fraud_rate\"]], palette=\"BuGn\")\n",
    "ax3.legend(loc='upper right')\n",
    "ax3.set(ylim=(0, 0.15))\n",
    "\n",
    "plt.ylabel('Fraud rate', axes=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** On the last days the fraud rate is unreliable due to label delay. Up until 2021-04-09 the fraud rate is a close enough approximation from the true fraud rate. The retrained model does have a bit higher volumes and larger volume-variability at the high-risk level compared to the stale model. On the first few days of the plot it does seem that to some degree the high-risk-volume of the daily-retrained-model might follow the fraud rate a bit better than the stale model, which might be good. This is limited evidence though, on so few days, and this analyses should be repeated on more data later to find a more reliable answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the volumes at high-risk better correlated with the fraud rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kendalltau(high_risk_results[\"fraud_rate\"], high_risk_results[\"stale_prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kendalltau(high_risk_results[\"fraud_rate\"], high_risk_results[\"retrained_prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Yes, while both the scores of the stale model and of the daily-retrained-model are only weakly correlated with the fraud rate, the daily-retrained model has slightly higher correlated to the fraud rate. This might mean that if on a certain day the model sends more properties to high risk, then with the daily-retrained-model there is a higher probability that this is because there really was more fraud on that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of volume-stability on the medium-risk threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat this analysis for the medium-risk volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medium_risk_results = nonspam_nontest_properties.groupby(\"registration_date\").agg(\n",
    "    stale_prediction=pd.NamedAgg(column='stale_prediction', aggfunc=lambda x: (x > 0.086).sum()),\n",
    "    retrained_prediction=pd.NamedAgg(column='retrained_prediction', aggfunc=lambda x: (x > 0.086).sum()),\n",
    "    fraud_rate=pd.NamedAgg(column='is_fake_hotel', aggfunc=lambda x: x.mean()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "y_axis_name = \"Number of medium-risk properties per day\"\n",
    "ax = sns.lineplot(\n",
    "    data=pd.melt(medium_risk_results.reset_index()[[\"registration_date\",\n",
    "                                                  \"stale_prediction\",\n",
    "                                                  \"retrained_prediction\"]],\n",
    "                 ['registration_date'],\n",
    "                 value_name=y_axis_name\n",
    "                ), \n",
    "    x=\"registration_date\", \n",
    "    y=y_axis_name, \n",
    "    hue='variable',\n",
    ")\n",
    "\n",
    "ax.set_title(\"The number of Medium-risk properties per day, according to daily-retrained model and the stale model.\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set(ylim=(0, 1500))\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(60)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax3 = sns.lineplot(data=medium_risk_results[[\"fraud_rate\"]], palette=\"BuGn\")\n",
    "ax3.legend(loc='upper right')\n",
    "ax3.set(ylim=(0, 0.15))\n",
    "\n",
    "plt.ylabel('Fraud rate', axes=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the volumes at medium-risk better correlated with the fraud rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kendalltau(medium_risk_results[\"fraud_rate\"], medium_risk_results[\"stale_prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kendalltau(medium_risk_results[\"fraud_rate\"], medium_risk_results[\"retrained_prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Surprisingly, the stale model even has a negative correlation between the fraud rate and the model score: the model assigns more medium-risk scores on days that there is less fraud! For the daily-retrained model there is a weak but positive correlation between the model score and the fraud rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of volume-stability on the experimental new high-risk threshold\n",
    "From 2021-04-20 we started a temporary experiment to be much more aggressive with fake property registrations and lower the high-risk threshold from 0.5 to 0.15. We now explore the stability of volumes under that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_high_risk_results = nonspam_nontest_properties.groupby(\"registration_date\").agg(\n",
    "    stale_prediction=pd.NamedAgg(column='stale_prediction', aggfunc=lambda x: (x > 0.15).sum()),\n",
    "    retrained_prediction=pd.NamedAgg(column='retrained_prediction', aggfunc=lambda x: (x > 0.15).sum()),\n",
    "    fraud_rate=pd.NamedAgg(column='is_fake_hotel', aggfunc=lambda x: x.mean()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "y_axis_name = \"Number of high-risk properties per day\"\n",
    "ax = sns.lineplot(\n",
    "    data=pd.melt(new_high_risk_results.reset_index()[[\"registration_date\",\n",
    "                                                  \"stale_prediction\",\n",
    "                                                  \"retrained_prediction\"]],\n",
    "                 ['registration_date'],\n",
    "                 value_name=y_axis_name\n",
    "                ), \n",
    "    x=\"registration_date\", \n",
    "    y=y_axis_name, \n",
    "    hue='variable',\n",
    ")\n",
    "\n",
    "ax.set_title(\"The number of High-risk properties per day, according to the new threshold of 0.15 with the daily-retrained model and the stale model.\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set(ylim=(0, 1500))\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(60)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax3 = sns.lineplot(data=medium_risk_results[[\"fraud_rate\"]], palette=\"BuGn\")\n",
    "ax3.legend(loc='upper right')\n",
    "ax3.set(ylim=(0, 0.15))\n",
    "\n",
    "plt.ylabel('Fraud rate', axes=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the volumes at the new high-risk level better correlated with the fraud rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kendalltau(new_high_risk_results[\"fraud_rate\"], new_high_risk_results[\"stale_prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kendalltau(new_high_risk_results[\"fraud_rate\"], new_high_risk_results[\"retrained_prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Again, the stale model even has a negative correlation between the fraud rate and the model score: the stale model at the new high-risk level assigns more high-risk scores on days that there is less fraud! For the daily-retrained model there is a weak but positive correlation between the model score and the fraud rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- We have seen evidence that the daily-retrained-model makes better predictions than the stale model that we currently run in production (RQ1)\n",
    "- We have not seen any bad deployments of our daily-retrained-model so far (RQ2)\n",
    "- At our current production thresholds, the volumnes of high-risk and of medium-risk properties are comparable (RQ3). Additionally, we see that under the daily-retrained model, the high-risk and medium-risk volumes correlate better with the actual fraud rate compared to under the stale model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Spark 2.4.4",
   "language": "python",
   "name": "spark244"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
