{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/blue-python/3.5/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Import Library \n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data & Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Data\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "combine = [train_df, test_df]\n",
    "\n",
    "\n",
    "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split() # Declare the columns names\n",
    "df_train = pd.read_csv(\"/home/yueyang/git_tree/cs_bi_psat_comment/data/psat_training_3k_1.tsv\",\n",
    "                       sep = \"\\t\", header = None, index_col = False, columns = columns) \n",
    "# sep = \"\\t\", sep = \",\", sep =\";\", \n",
    "# 2 rows of header : header = [0,1], 1 row of header : header = 0\n",
    "# with index : index_col = 0\n",
    "df['date'] = pd.to_datetime(df['yyyy_mm_dd'])\n",
    "\n",
    "\n",
    "# save data\n",
    "output_file = '/home/yueyang/git_tree/cs_bi_psat_comment/data/psat_comments.csv'\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train test data\n",
    "data = df.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "## direct set 1/3 for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)\n",
    "\n",
    "## K-fold split\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "# the more folds we have, we will be reducing the error due the bias but increasing the error due to variance; \n",
    "# the computational price would go up too, obviously — the more folds you have, the longer it would take to compute it\n",
    "# and you would need more memory.\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=False) # Define the split - into 5 folds \n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "for train_index, test_index in kf.split(X):\n",
    "#    print(“TRAIN:”, train_index, “TEST:”, test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=False) # Define the split - into 5 folds \n",
    "kf.split(df), None\n",
    "for train_index, test_index in kf.split(df):\n",
    "#    print(“TRAIN:”, train_index, “TEST:”, test_index)\n",
    "    train_df, test_df = df.iloc[train_index], df.iloc[test_index]\n",
    "    \n",
    "\n",
    "    # Or CV split   \n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3\n",
    "                                        , train_size = .6, random_state = 0 ) \n",
    "# run model 10x with 60/30 split intentionally leaving out 10%\n",
    "    \n",
    "# drop non-relevant features for prediction\n",
    "train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "\n",
    "# drop target column\n",
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n",
    "X_train.shape, Y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Descriptive Statistics\n",
    "print(df.columns.values)\n",
    "df.info()\n",
    "print('_'*40)\n",
    "\n",
    "df.describe()\n",
    "df.describe(include=['O'])\n",
    "df[['col1','cl2']].describe()\n",
    "df['categorical'].unique()\n",
    "df['categorical'].value_counts()\n",
    "\n",
    "\n",
    "# NULL values\n",
    "df.isnull().sum()\n",
    "# select non-null rows\n",
    "df_train = df_train[(~df_train['col1'].isnull()) & \n",
    "                    (~df_train['col2'].isnull())& \n",
    "                    (~df_train['col3'].isnull())]\n",
    "# fill na with text\n",
    "y_true = df['label_topic'].fillna('unknown')\n",
    "\n",
    "# fill na with median\n",
    "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n",
    "\n",
    "# Reset Index\n",
    "data = data.reset_index(drop=True)\n",
    "data['index_num'] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# column value in a list\n",
    "age_group =[\"mature: > 2 yrs\", \"new: < 7d\"]\n",
    "seg_group = [\"SPO\", \"MUP\", \"Not Home\",\"Local MPP\"]\n",
    "df = df[(df['property_partner_age'].isin(age_group)) & \n",
    "                       (df['groups_bhome_segmentation'].isin(seg_group))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Visualization\n",
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "ax = df['word_count'].hist(bins=np.arange(0, 50))\n",
    "ax.set_xlabel('Word Count per Comment (1 word each bin)')\n",
    "ax.set_ylabel('Comments Count')\n",
    "ax.set_title('Word Count Distribution per Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot value counts\n",
    "# Method 1\n",
    "col_counts = df['categorical'].value_counts()\n",
    "col_counts.plot(kind='bar')\n",
    "plt.xticks(rotation=25)\n",
    "\n",
    "# Mehtod 2\n",
    "topic_size = df.groupby(['col']).size().reset_index(name = 'topic_size')\n",
    "topic_size = topic_size.sort_values(by = 'topic_size', ascending = False)\n",
    "\n",
    "plt.figure(figsize=(4,3), dpi=100)\n",
    "ax = sns.barplot(y=\"col\", x=\"topic_size\", data=topic_size, \n",
    "                 palette=\"Blues_d\")\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "    \n",
    "# Method 3\n",
    "sns.set(font_scale=1.2)  # crazy big\n",
    "plt.figure(figsize=(3,4), dpi=100)\n",
    "ax = sns.countplot(y=\"model_topic\", data=plt_data, order = plt_data['model_topic'].value_counts().index )\n",
    "\n",
    "# Method 3 for facetgrid\n",
    "sns.set(font_scale=1.4)  # crazy big\n",
    "plt.figure(figsize=(3,2), dpi=25)\n",
    "\n",
    "#plt_data = df\n",
    "plt_data = df[df['model_parent_topic']=='payments']\n",
    "plt_data.rename(columns={'bhome_segmentation': 'seg'}, inplace=True)\n",
    "ax = sns.catplot(y = \"model_topic\", data= plt_data, col = 'seg',\n",
    "#                 col_wrap=2,\n",
    "                 col_order = ['SPO','MUP','Not Home','Local MPP'],\n",
    "                  order = plt_data['model_topic'].value_counts().index, kind = 'count', sharex=False)\n",
    "ax.set_axis_labels(\"Count\", \"Topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target variable is split as 2 graphs\n",
    "# get the age distribution in 2 values of the categorical value\n",
    "g = sns.FacetGrid(train_df, col='Survived')\n",
    "g.map(plt.hist, 'Age', bins=20)\n",
    "\n",
    "\n",
    "grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "grid.add_legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram of numerical value with categorical as A/B in 1 figure\n",
    "#plot distributions of age of passengers who survived or did not survive\n",
    "a = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\n",
    "a.map(sns.kdeplot, 'Age', shade= True )\n",
    "a.set(xlim=(0 , data1['Age'].max()))\n",
    "a.add_legend()\n",
    "\n",
    "\n",
    "#histogram comparison of sex, class, and age by survival\n",
    "h = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\n",
    "h.map(plt.hist, 'Age', alpha = .75)\n",
    "h.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# heatmap\n",
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by & Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group by 2 categorical columns and \n",
    "train_df[['categorical_1', 'categorical_2']].groupby(['categorical_1']\n",
    "                                         , as_index=False).mean().sort_values(by='categorical_2', ascending=False)\n",
    "\n",
    "# Group by 2 categorical columns and get the count of each sub category\n",
    "pd.crosstab(df['categorical_1'], [df['categorical_2']], rownames = ['categorical_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical value to band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numerical value to band\n",
    "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\n",
    "train_df[['AgeBand', 'Survived']].groupby(['AgeBand'],\n",
    "                                          as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n",
    "\n",
    "for dataset in combine:    \n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical value to numerical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical value to numerical value \n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary \n",
    "parent_topic_dict = dict.fromkeys([0, 3, 7, 12], \"waiting for response\")\n",
    "parent_topic_dict.update(dict.fromkeys([1], \"cancellation\"))\n",
    "df['model_parent_topic'] = df['topic_num'].map(parent_topic_dict)\n",
    "\n",
    "topic_dict = dict.fromkeys([0], \"wait for call back, wait long on the phone and dropped call\")\n",
    "topic_dict.update(dict.fromkeys([1], \"complaint about guest cancellation & no show\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply and list function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply \n",
    "def pre_processing(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "df['pre_text']= df['text'].apply(lambda text: pre_processing(text))\n",
    "\n",
    "\n",
    "# \n",
    "agg_func = lambda s: {w:s for w, s in zip(s[\"topic_words\"].values.tolist(),                                        \n",
    "                                                    s[\"word_scores\"].values.tolist())}\n",
    "self.grouped = self.data.groupby(\"topic_num\").apply(agg_func)\n",
    "self.keywords = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyword_df = pd.DataFrame()\n",
    "for i in range(0, num_topic):\n",
    "    keyword_df_tmp = pd.DataFrame()\n",
    "    keyword_df_tmp[\"topic_words\"] = topic_words[i][0:9]\n",
    "    keyword_df_tmp[\"word_scores\"] = word_scores[i][0:9]\n",
    "    keyword_df_tmp['topic_num'] = i\n",
    "    keyword_df = pd.concat([keyword_df, keyword_df_tmp])\n",
    "keyword_df\n",
    "\n",
    "\n",
    "org_df = pd.DataFrame(matches, columns = ['type', 'start', 'end','ratio'])\n",
    "org_df['word'] = [doc[s:e] for (s, e) in zip(org_df['start'], org_df['end'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featuer Scalling\n",
    "https://stats.stackexchange.com/questions/244507/what-algorithms-need-feature-scaling-beside-from-svm\n",
    "\n",
    "sensitive machine learning to Scalling:\n",
    "KNN, SGD, SVm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scalling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "\n",
    "## transforming \"train_x\"\n",
    "X_train = std_scale.fit_transform(X_train)\n",
    "## transforming \"test_x\"\n",
    "X_test = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = data1[Target]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[feature_columns], data1[Target], cv  = cv_split)\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1[feature_columns], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[feature_columns])\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict(X_test)\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "acc_log\n",
    "\n",
    "# get coefficient\n",
    "coeff_df = pd.DataFrame(train_df.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "Y_pred = svc.predict(X_test)\n",
    "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "Y_pred = gaussian.predict(X_test)\n",
    "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n",
    "acc_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, Y_train)\n",
    "Y_pred = sgd.predict(X_test)\n",
    "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
    "acc_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Foerest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# fit the model\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "random_forest.score(X_train, Y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear regression\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "predictions[0:5]\n",
    "\n",
    "# plot the model \n",
    "## The line / model\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel(“True Values”)\n",
    "plt.ylabel(“Predictions”)\n",
    "\n",
    "\n",
    "## print accuracy\n",
    "print “Score:”, model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# fit the model\n",
    "model = RandomForestRegressor(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_plot(topic_confusion):\n",
    "    for i in range(len(topic_confusion)):\n",
    "        row = topic_confusion.iloc[i,:]\n",
    "        print('Class:    {}\\nAccuracy: {}%'.format(topic_confusion.columns.values[i], round(row[i] / sum(row) * 100, 2)))\n",
    "        c = ['black' for i in list(range(len(row)))]\n",
    "        c[i] = 'green'\n",
    "\n",
    "        objects = list(row.index)\n",
    "        y_pos = np.arange(len(objects))\n",
    "        predictions = list(row)\n",
    "        plt.bar(y_pos, predictions, align='center', alpha=0.5, color=c)\n",
    "        plt.xticks(y_pos, objects, rotation=90)\n",
    "        plt.ylabel('Predicted Class Count')\n",
    "        plt.title('Actual Class: {}'.format(row.name))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_parent_true = df['label_parent_topic']\n",
    "y_parent_pred = df['model_parent_topic']\n",
    "labels = np.unique(y_parent_true)\n",
    "confusion_parent = pd.DataFrame(confusion_matrix(y_parent_true, y_parent_pred, labels = labels),\n",
    "                                index = labels, columns = labels)\n",
    "print('Parent Topic Confusion Matrix\\n')\n",
    "print(confusion_parent)\n",
    "confusion_plot(confusion_parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report (F1 score, precision, recall, support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('\\nParent Topic Classification Report\\n')\n",
    "print(classification_report(y_parent_true, y_parent_pred))\n",
    "\n",
    "\n",
    "def classification_report_csv(report):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = row_data[-5]\n",
    "        row['precision'] = float(row_data[-4])\n",
    "        row['recall'] = float(row_data[-3])\n",
    "        row['f1_score'] = float(row_data[-2])\n",
    "        row['support'] = float(row_data[-1])\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    #dataframe.to_csv('classification_report.csv', index = False)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "report_parent = classification_report(y_parent_true, y_parent_pred)\n",
    "df_report_parent = classification_report_csv(report_parent)\n",
    "df_report_parent = df_report_parent.sort_values(by = 'f1_score', ascending = False)\n",
    "df_report_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the classification report\n",
    "import plot_heatmap as hp\n",
    "hp.plot_classification_report(report_parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, F1 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('Weighted F1-score : ', f1_score(y_parent_true, y_parent_pred, average='weighted'))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "\n",
    "# accuracy with cross validation: \n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "# Perform 6-fold cross validation\n",
    "scores = cross_val_score(model, df, y, cv=6)\n",
    "print \"Cross-validated scores:\", scores\n",
    "\n",
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model, df, y, cv=6)\n",
    "plt.scatter(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "data_plt = df_report[df_report['f1_score']>=0.68]\n",
    "class_names = [str(r) for r in data_plt['class']]\n",
    "support = data_plt['support'].astype(int)\n",
    "sns.set(font_scale=1.2)\n",
    "g = sns.heatmap(\n",
    "      data_plt['f1_score'].values.reshape(10, 1),\n",
    "      xticklabels=['F1 Score'],\n",
    "#      yticklabels = [str(r) for r in data_plt['class']],\n",
    "      yticklabels =['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)],\n",
    "      cmap=\"RdBu\"\n",
    ")\n",
    "g.set_title('Top 10 most accurate preditced topics \\n F1 Score >= 68% ', fontsize=20)\n",
    "#g.set_xlabel('Input Sentence', color = 'b', fontsize = 15)\n",
    "g.tick_params(axis='x', colors='b')\n",
    "g.set_ylabel('Detect Topics',fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC & ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "#plt.style.use('seaborn-pastel')\n",
    "y_score = model.decision_function(X_test)\n",
    "\n",
    "FPR, TPR, _ = roc_curve(y_test, y_score)\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "print (ROC_AUC)\n",
    "\n",
    "plt.figure(figsize =[11,9])\n",
    "plt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\n",
    "plt.plot([0,1],[0,1], 'k--', linewidth = 4)\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize = 18)\n",
    "plt.ylabel('True Positive Rate', fontsize = 18)\n",
    "plt.title('ROC for Titanic survivors', fontsize= 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_n_estimator = [10, 50, 100, 300]\n",
    "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "grid_learn = [.01, .03, .05, .1, .25]\n",
    "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "grid_min_samples = [5, 10, .03, .05, .10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vote_est = [\n",
    "    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc',ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    \n",
    "    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hard Vote or majority rules\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, data1[feature_columns], data1[Target], cv  = cv_split)\n",
    "vote_hard.fit(data1[feature_columns], data1[Target])\n",
    "\n",
    "print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#Soft Vote or weighted probabilities\n",
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, data1[feature_columns], data1[Target], cv  = cv_split)\n",
    "vote_soft.fit(data1[feature_columns], data1[Target])\n",
    "\n",
    "print(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_param = [\n",
    "            [{\n",
    "            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'n_estimators': grid_n_estimator, #default=50\n",
    "            'learning_rate': grid_learn, #default=1\n",
    "            #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "       \n",
    "    \n",
    "            [{\n",
    "            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'max_samples': grid_ratio, #default=1.0\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=”gini”\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "\n",
    "            [{\n",
    "            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            #'loss': ['deviance', 'exponential'], #default=’deviance’\n",
    "            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n",
    "            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n",
    "            'max_depth': grid_max_depth, #default=3   \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=”gini”\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{    \n",
    "            #GaussianProcessClassifier\n",
    "            'max_iter_predict': grid_n_estimator, #default: 100\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "        \n",
    "    \n",
    "            [{\n",
    "            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'fit_intercept': grid_bool, #default: True\n",
    "            #'penalty': ['l1','l2'],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "            \n",
    "    \n",
    "            [{\n",
    "            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'alpha': grid_ratio, #default: 1.0\n",
    "             }],\n",
    "    \n",
    "    \n",
    "            #GaussianNB - \n",
    "            [{}],\n",
    "    \n",
    "            [{\n",
    "            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n",
    "            'weights': ['uniform', 'distance'], #default = ‘uniform’\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }],\n",
    "            \n",
    "    \n",
    "            [{\n",
    "            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [1,2,3,4,5], #default=1.0\n",
    "            'gamma': grid_ratio, #edfault: auto\n",
    "            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'learning_rate': grid_learn, #default: .3\n",
    "            'max_depth': [1,2,4,6,8,10], #default 2\n",
    "            'n_estimators': grid_n_estimator, \n",
    "            'seed': grid_seed  \n",
    "             }]   \n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
    "for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n",
    "\n",
    "    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n",
    "    #print(param)\n",
    "    \n",
    "    \n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
    "    best_search.fit(data1[feature_columns], data1[Target])\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n",
    "\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "rfc_feature = rfc.feature_importances(x_train,y_train)\n",
    "gbc_feature = gbc.feature_importances(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = train.columns.values\n",
    "# Create a dataframe with features\n",
    "feature_dataframe = pd.DataFrame( {'features': cols,\n",
    "     'Random Forest feature importances': rf_features,\n",
    "    'Gradient Boost feature importances': gb_features\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot \n",
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['Random Forest feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "#       size= feature_dataframe['AdaBoost feature importances'].values,\n",
    "        #color = np.random.randn(500), #set color equal to a variable\n",
    "        color = feature_dataframe['Random Forest feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Random Forest Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "#     xaxis= dict(\n",
    "#         title= 'Pop',\n",
    "#         ticklen= 5,\n",
    "#         zeroline= False,\n",
    "#         gridwidth= 2,\n",
    "#     ),\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')\n",
    "\n",
    "\n",
    "# Scatter plot \n",
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['Gradient Boost feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "#       size= feature_dataframe['AdaBoost feature importances'].values,\n",
    "        #color = np.random.randn(500), #set color equal to a variable\n",
    "        color = feature_dataframe['Gradient Boost feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Gradient Boosting Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "#     xaxis= dict(\n",
    "#         title= 'Pop',\n",
    "#         ticklen= 5,\n",
    "#         zeroline= False,\n",
    "#         gridwidth= 2,\n",
    "#     ),\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensembelling\n",
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bagging Classifier and Bagging Regressor from Sklearn\n",
    "# train is the training data\n",
    "# test is the test data\n",
    "# y is the target variable\n",
    "model = RandomForestRegressor()\n",
    "bags = 10\n",
    "seed = 1\n",
    "\n",
    "#create array object to hold bagged predictions\n",
    "bagged_prediction = np.zeros(test.shape[0])\n",
    "\n",
    "#loop for as many times as we want bags\n",
    "for n in range (0, bags):\n",
    "    model.set_params(random_state = seed +n) #update seed\n",
    "    model.fit(train, y)\n",
    "    preds = model.predict(test) #predict on test data\n",
    "    bagged_prediction += preds # add predictions to bagged predictions\n",
    "\n",
    "# take average of predictions\n",
    "bagged_prediction /= bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sklearn Bagging\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# define the model\n",
    "model = BaggingClassifier()\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 base learners\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train is the training data\n",
    "# y is the target variable for the train data\n",
    "# test is the test data\n",
    "\n",
    "#split train data in 2 parts, training and validation. \n",
    "training, valid, ytraining, yvalid = train_test_split(train, y.test_size = 0.5)\n",
    "\n",
    "#specify models\n",
    "model1 = RandomForestRegressor()\n",
    "model2 = LinearRegression()\n",
    "\n",
    "#fit models\n",
    "model1.fit(training, ytraining)\n",
    "model2.fit(training, ytraining)\n",
    "\n",
    "#make predictions for validation\n",
    "preds1 = model1.predict(valid)\n",
    "preds2 = model2.predict(valid)\n",
    "\n",
    "# make predictions for test data\n",
    "test_preds1 = model1.predict(test)\n",
    "test_preds2 = model2.predict(test)\n",
    "\n",
    "# form a new dataset for valid and test via stacking the predictions\n",
    "stacked_predictions = np.column_stack((preds1, preds2))\n",
    "stacked_test_predictions = np.column_stack((test_preds1, test_preds2))\n",
    "\n",
    "#specify meta model\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_predictions, yvalid)\n",
    "#make predictions on the stacked predictions of the test data\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
